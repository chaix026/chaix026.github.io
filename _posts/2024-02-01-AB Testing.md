---
title: "A/B Testing"
excerpt: "Identify changes that lead to improvements"
categories:
  - portfolio
tags:
  - python
  - causal inference
  - data science
---
![AB Testing](/assets/images/portfolio/Slide1.JPG)  
A/B testing, also known as split testing, is a common practice in marketing ad campaigns aimed at optimizing conversion rates and maximizing performance. It involves comparing two versions of a marketing asset, such as an advertisement or a landing page, by randomly assigning users to one of the versions and measuring their response. By analyzing the results, marketers can identify which version performs better in terms of predefined metrics, such as click-through rates, conversion rates, or user engagement. A/B testing provides valuable insights into the effectiveness of different marketing strategies, allowing marketers to make data-driven decisions and refine their campaigns to achieve the desired outcomes.

### Data
In this notebook, I demonstrate an A/B testing using Python for an example ad campaign dataset on Kaggle where the majority of the people will be exposed to ads (the experimental group). And a small portion of people (the control group) would instead see a Public Service Announcement (PSA) (or nothing) in the exact size and place the ad would normally be.

The idea of the dataset is to analyze the groups, find if the ads were successful, how much the company can make from the ads, and if the difference between the groups is statistically significant.

[Code](https://github.com/chaix026/A-B-Testing){: .btn .btn--success .btn--large}

